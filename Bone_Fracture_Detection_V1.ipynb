{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bone Fracture Detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abntrPUaqw3u",
        "outputId": "31b8d351-d3fe-407b-bc42-00868ac735e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Bone Fracture Detection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"/content/drive/My Drive/MURA-v1.1.zip\"\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print(\"Completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9soIvCc_rBTt",
        "outputId": "3f8ed0ae-3656-49dd-cc4e-6d4b94eb43b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
        "from keras.preprocessing.image import * #image\n",
        "from keras import regularizers\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.applications.densenet import DenseNet169, DenseNet121, preprocess_input\n",
        "from time import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import datetime"
      ],
      "metadata": {
        "id": "rqrFOecPrTIA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install focal-loss\n",
        "!pip install keras-radam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B70kMigftSrI",
        "outputId": "6212ac50-a4e3-41f9-b71a-7cc4632b715f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: focal-loss in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.7/dist-packages (from focal-loss) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.10.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.37.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.13.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.19.5)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.12.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.17.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (12.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.7.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.43.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2->focal-loss) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (3.1.1)\n",
            "Requirement already satisfied: keras-radam in /usr/local/lib/python3.7/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-radam) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-radam) (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from focal_loss import BinaryFocalLoss\n",
        "from keras_radam import RAdam\n"
      ],
      "metadata": {
        "id": "5UzHFeCatZ18"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def listdir_nohidden(path):\n",
        "    '''\n",
        "    Utility function to find the list of files in a directory excluding the hidden files.\n",
        "    Args:\n",
        "        path: contains the path of the directory containing the images\n",
        "\n",
        "    '''\n",
        "    for f in os.listdir(path):\n",
        "        if not f.startswith('.'):\n",
        "            yield f"
      ],
      "metadata": {
        "id": "YsBb39OwtdgC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_images_metadata_csv(category,study_types):\n",
        "    '''\n",
        "    This function creates a csv file containing the path of images, label.\n",
        "    Args:\n",
        "        category: train or valid, depending on which csv is needed\n",
        "        study_types: the type of the body part in MURA Dataset, eg:  XR_SHOULDER\n",
        "    '''\n",
        "    image_data = {}\n",
        "    study_label = {'positive': 1, 'negative': 0}\n",
        "    #study_types = ['XR_ELBOW','XR_FINGER','XR_FOREARM','XR_HAND','XR_HUMERUS','XR_SHOULDER','XR_WRIST']\n",
        "    #study_types = ['XR_ELBOW']\n",
        "    i = 0\n",
        "    image_data[category] = pd.DataFrame(columns=['Path','Count', 'Label'])\n",
        "    for study_type in study_types: # Iterate throught every study types\n",
        "        DATA_DIR = 'MURA-v1.1/%s/%s/' % (category, study_type)\n",
        "        patients = list(os.walk(DATA_DIR))[0][1]  # list of patient folder names\n",
        "        for patient in tqdm(patients):  # for each patient folder\n",
        "            for study in os.listdir(DATA_DIR + patient):  # for each study in that patient folder\n",
        "                if(study != '.DS_Store'):\n",
        "                    label = study_label[study.split('_')[1]]  # get label 0 or 1\n",
        "                    path = DATA_DIR + patient + '/' + study + '/'  # path to this study\n",
        "                    for j in range(len(list(listdir_nohidden(path)))):\n",
        "                        image_path = path + 'image%s.png' % (j + 1)\n",
        "                        image_data[category].loc[i] = [image_path,1, label]  # add new row\n",
        "                        i += 1\n",
        "    image_data[category].to_csv(category+\"_image_data.csv\",index = None, header=False)"
      ],
      "metadata": {
        "id": "u9UtTZ80urT6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getImageLabels(dataframe):\n",
        "  '''\n",
        "  This function is to get the labels of the images from the dataframe\n",
        "  Args:\n",
        "      dataframe: pandas dataframe containing the labels of the images\n",
        "\n",
        "  '''\n",
        "  labels = []\n",
        "  for i, data in tqdm(dataframe.iterrows()):\n",
        "\n",
        "      labels.append(data['Label'])\n",
        "\n",
        "  labels = np.asarray(labels)\n",
        "  return labels"
      ],
      "metadata": {
        "id": "RLIeZQX5vOTn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getImage(dataframe, size):\n",
        "    '''\n",
        "    Function to process the images\n",
        "    Args:\n",
        "        dataframe: contains the path to the images in the directory\n",
        "        size: contains the value to which the shape of the image will resized\n",
        "    '''\n",
        "    Images = []\n",
        "    for i, data in tqdm(dataframe.iterrows()):\n",
        "      try:\n",
        "        image = cv2.imread(data['Path'])#cv2.IMREAD_GRAYSCALE)\n",
        "        image = cv2.resize(image,(size,size))\n",
        "        #image = randome_rotation_flip(image,size)\n",
        "        Images.append(image)\n",
        "\n",
        "      except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "    Images = np.asarray(Images).astype('float32')\n",
        "\n",
        "    mean = np.mean(Images)\t\t\t#normalization\n",
        "    std = np.std(Images)\n",
        "    Images = (Images - mean) / std\n",
        "    \n",
        "  \n",
        "    return Images"
      ],
      "metadata": {
        "id": "vhi99qs9vTa9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    base_model = DenseNet169(input_shape=(None, None,3),\n",
        "                             weights='imagenet',\n",
        "                             include_top=False,\n",
        "                             pooling='avg')\n",
        "\n",
        "\n",
        "    x = base_model.output\n",
        "\n",
        "    predictions = Dense(n_classes,activation='sigmoid')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model"
      ],
      "metadata": {
        "id": "OV_HggPmvZAa"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DATA PROCESSING**"
      ],
      "metadata": {
        "id": "1fOL4BWwOYn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height = 224, 224\n",
        "#Keras ImageDataGenerator to load, transform the images of the dataset\n",
        "\n",
        "IMG_DATA_DIR = 'MURA-v1.1/'\n",
        "train_data_dir = IMG_DATA_DIR + 'train/XR_HUMERUS'\n",
        "valid_data_dir = IMG_DATA_DIR + 'valid/XR_HUMERUS'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    horizontal_flip=True\n",
        "\n",
        ")\n",
        "study_types = ['XR_HUMERUS']\n",
        "\n",
        "#TRAIN DATA\n",
        "create_images_metadata_csv('train',study_types)\n",
        "#VALID DATA\n",
        "create_images_metadata_csv('valid',study_types)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzqifeWhMyJn",
        "outputId": "90fdfd18-6813-42bf-8a5a-5a30131b3670"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 587/587 [00:05<00:00, 116.90it/s]\n",
            "100%|██████████| 132/132 [00:01<00:00, 124.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_image_df = pd.read_csv('valid_image_data.csv', names=['Path','Count', 'Label'])\n",
        "train_image_df = pd.read_csv('train_image_data.csv', names=['Path', 'Count','Label'])\n",
        "\n",
        "dd={}\n",
        "\n",
        "dd['train'] = train_image_df\n",
        "dd['valid'] = valid_image_df"
      ],
      "metadata": {
        "id": "NJc5lmzuOXWl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_labels = getImageLabels(train_image_df)\n",
        "train_images = getImage(train_image_df, size = 224)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXsggYkLOv9W",
        "outputId": "4e8ff8c4-774a-4fa5-f2ae-f6abc485f646"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1272it [00:00, 7327.85it/s]\n",
            "1272it [00:05, 242.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape\n",
        "train_datagen.fit(train_images,augment=True)\n",
        "train_generator = train_datagen.flow(\n",
        "    x=train_images,\n",
        "    y=train_image_labels,\n",
        "    batch_size = 32\n",
        ")"
      ],
      "metadata": {
        "id": "ccvEfKVGO4s8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_image_labels = getImageLabels(valid_image_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opAqlQkUPFii",
        "outputId": "68c1bbf9-471c-4934-ee35-f2c9434782ac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "288it [00:00, 8148.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_images = getImage(valid_image_df, size = 224)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWOXLUwWPJDF",
        "outputId": "b7f6cbaf-924c-4da4-e2de-a11272451023"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "288it [00:01, 209.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen.fit(valid_images,augment=True)"
      ],
      "metadata": {
        "id": "8tIr0vsKPMup"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_generator = test_datagen.flow(\n",
        "    x=valid_images,\n",
        "    y=valid_image_labels,\n",
        "    batch_size = 1\n",
        ")"
      ],
      "metadata": {
        "id": "osmRPYRlPQeb"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}